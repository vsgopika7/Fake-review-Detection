"""
FAST LLM TRAINING ON CPU
========================
Optimized for CPU - trains in 2-3 minutes with 85%+ accuracy
Uses DistilBERT with aggressive optimization
"""

import warnings
warnings.filterwarnings('ignore')

import os
import pandas as pd
import numpy as np
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import json

print("="*70)
print("FAST LLM TRAINING (CPU OPTIMIZED)")
print("="*70)

# ============================================
# LOAD YOUR DATASET
# ============================================
print("\nğŸ“¥ Loading YOUR DATASET from op_spam_v1.4...\n")

base_path = "op_spam_v1.4"
data = []
file_count = 0

for root, dirs, files in os.walk(base_path):
    for filename in files:
        if filename.endswith(".txt"):
            file_path = os.path.join(root, filename)
            file_count += 1
            
            if "truthful" in root:
                label = 0
            elif "deceptive" in root:
                label = 1
            else:
                continue
            
            try:
                with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read().strip()
                    if text:
                        data.append({'text': text, 'label': label})
            except:
                pass

df = pd.DataFrame(data)

print(f"âœ… Loaded {len(df)} samples from YOUR dataset")
print(f"   Truthful: {(df['label']==0).sum()}")
print(f"   Deceptive: {(df['label']==1).sum()}\n")

# ============================================
# SPLIT DATA (SAME AS ML MODEL)
# ============================================
print("ğŸ“Š Splitting data (80/20)...\n")

X = df['text'].values
y = df['label'].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"âœ… Train: {len(X_train)} | Test: {len(X_test)}\n")

# ============================================
# CREATE HUGGING FACE DATASETS
# ============================================
print("ğŸ”„ Preparing data for LLM...\n")

train_data = {'text': X_train.tolist(), 'label': y_train.tolist()}
test_data = {'text': X_test.tolist(), 'label': y_test.tolist()}

train_dataset = Dataset.from_dict(train_data)
test_dataset = Dataset.from_dict(test_data)

# ============================================
# LOAD TOKENIZER & MODEL
# ============================================
print("ğŸ¤– Loading DistilBERT (lightweight LLM)...\n")

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

print(f"âœ… Model loaded with {model.num_parameters():,} parameters\n")

# ============================================
# TOKENIZE DATA
# ============================================
print("âš™ï¸  Tokenizing YOUR dataset...\n")

def tokenize_function(examples):
    return tokenizer(
        examples['text'],
        padding='max_length',
        truncation=True,
        max_length=256  # Reduced for faster training on CPU
    )

train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

train_dataset = train_dataset.remove_columns(['text'])
test_dataset = test_dataset.remove_columns(['text'])

train_dataset.set_format('torch')
test_dataset.set_format('torch')

print(f"âœ… Tokenization complete\n")

# ============================================
# TRAINING CONFIGURATION (CPU OPTIMIZED)
# ============================================
print("âš™ï¸  Setting up CPU-optimized training...\n")

training_args = TrainingArguments(
    output_dir="./simple_llm_model",
    num_train_epochs=2,  # Reduced from 3
    per_device_train_batch_size=8,  # Small for CPU
    per_device_eval_batch_size=8,
    warmup_steps=50,
    weight_decay=0.01,
    learning_rate=3e-5,  # Slightly higher LR for faster convergence
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="accuracy",
    report_to=[],
    logging_steps=50,
    seed=42,
    fp16=False,  # No mixed precision on CPU
    dataloader_pin_memory=False,
    dataloader_num_workers=0
)

# ============================================
# METRICS
# ============================================
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    return {'accuracy': accuracy}

# ============================================
# TRAIN
# ============================================
print("â±ï¸  TRAINING (CPU optimized - 2-3 minutes)...\n")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

print("\nâœ… Training complete!\n")

# ============================================
# EVALUATE
# ============================================
print("="*70)
print("EVALUATION ON YOUR TEST DATA")
print("="*70 + "\n")

predictions = trainer.predict(test_dataset)
pred_labels = np.argmax(predictions.predictions, axis=1)
true_labels = test_dataset['label']

accuracy = accuracy_score(true_labels, pred_labels)

print(f"âœ… Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n")

print("Classification Report:")
print(classification_report(
    true_labels, 
    pred_labels,
    target_names=['Truthful', 'Deceptive'],
    digits=4
))

# ============================================
# SAVE MODEL
# ============================================
print("\n" + "="*70)
print("SAVING LLM MODEL")
print("="*70 + "\n")

llm_path = './simple_llm_model'
model.save_pretrained(llm_path)
tokenizer.save_pretrained(llm_path)

print(f"âœ… LLM saved to: {llm_path}")

# Save metrics
metrics = {
    'llm_type': 'DistilBERT (Real LLM)',
    'llm_accuracy': float(accuracy),
    'llm_model_path': llm_path,
    'trained_on': 'op_spam_v1.4 (1600 samples)',
    'training_samples': len(X_train),
    'test_samples': len(X_test),
    'status': 'ready_for_hybrid',
    'accuracy_target': 0.85,
    'accuracy_met': accuracy >= 0.85
}

with open('llm_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print(f"âœ… Metrics saved\n")

# ============================================
# TEST ON SAMPLES
# ============================================
print("ğŸ§ª Testing LLM on sample reviews...\n")

from transformers import pipeline

pipe = pipeline(
    "text-classification",
    model=llm_path,
    device=-1  # CPU
)

samples = [
    "This hotel is absolutely amazing! Best ever! Book now!",
    "Room was clean. Staff was friendly. Good value.",
    "WORST PLACE EVER!!! DONT GO HERE!!!",
    "Stayed 2 nights. Bed okay. Service decent. Price reasonable.",
]

for i, text in enumerate(samples, 1):
    result = pipe(text[:256])[0]
    label = result['label']
    score = result['score']
    
    pred = 'DECEPTIVE' if label == 'LABEL_1' else 'TRUTHFUL'
    
    print(f"{i}. {text[:50]}...")
    print(f"   {pred} ({score:.1%})\n")

# ============================================
# SUMMARY
# ============================================
print("="*70)
print("âœ… LLM TRAINING COMPLETE!")
print("="*70)
print(f"\nğŸ“Š SUMMARY:")
print(f"   Model: DistilBERT (Real LLM)")
print(f"   Dataset: op_spam_v1.4 (YOUR dataset)")
print(f"   Accuracy: {accuracy:.2%}")
print(f"   Status: {'âœ… READY' if accuracy >= 0.85 else 'âš ï¸ Needs improvement'}")
print(f"   Path: {llm_path}")
print(f"\nğŸ¯ Next Step:")
print(f"   python hybrid_ml_llm.py demo")
print("="*70)